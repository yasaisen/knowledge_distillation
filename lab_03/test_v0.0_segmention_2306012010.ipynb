{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "# base path of the dataset\n",
    "DATASET_PATH = os.path.join(\"dataset\", \"train\")\n",
    "# define the path to the images and masks dataset\n",
    "# IMAGE_DATASET_PATH = os.path.join(DATASET_PATH, \"images\")\n",
    "# MASK_DATASET_PATH = os.path.join(DATASET_PATH, \"masks\")\n",
    "# define the test split\n",
    "TEST_SPLIT = 0.15\n",
    "# determine the device to be used for training and evaluation\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# determine if we will be pinning memory during data loading\n",
    "PIN_MEMORY = True if DEVICE == \"cuda\" else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of channels in the input, number of classes,\n",
    "# and number of levels in the U-Net model\n",
    "NUM_CHANNELS = 1\n",
    "NUM_CLASSES = 1\n",
    "NUM_LEVELS = 3\n",
    "# initialize learning rate, number of epochs to train for, and the\n",
    "# batch size\n",
    "INIT_LR = 0.001\n",
    "NUM_EPOCHS = 40\n",
    "BATCH_SIZE = 64\n",
    "# define the input image dimensions\n",
    "INPUT_IMAGE_WIDTH = 128\n",
    "INPUT_IMAGE_HEIGHT = 128\n",
    "# define threshold to filter weak predictions\n",
    "THRESHOLD = 0.5\n",
    "# define the path to the base output directory\n",
    "BASE_OUTPUT = \"output\"\n",
    "# define the path to the output serialized model, model training\n",
    "# plot, and testing image paths\n",
    "MODEL_PATH = os.path.join(BASE_OUTPUT, \"unet_tgs_salt.pth\")\n",
    "PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])\n",
    "TEST_PATHS = os.path.sep.join([BASE_OUTPUT, \"test_paths.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/yasaisen/Desktop/13_research/research_main/lab_00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>directory</th>\n",
       "      <th>images</th>\n",
       "      <th>masks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA_HT_7690_19960312</td>\n",
       "      <td>TCGA_HT_7690_19960312_16.tif</td>\n",
       "      <td>TCGA_HT_7690_19960312_16_mask.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA_HT_7690_19960312</td>\n",
       "      <td>TCGA_HT_7690_19960312_10.tif</td>\n",
       "      <td>TCGA_HT_7690_19960312_10_mask.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA_HT_7690_19960312</td>\n",
       "      <td>TCGA_HT_7690_19960312_18.tif</td>\n",
       "      <td>TCGA_HT_7690_19960312_18_mask.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA_HT_7690_19960312</td>\n",
       "      <td>TCGA_HT_7690_19960312_17.tif</td>\n",
       "      <td>TCGA_HT_7690_19960312_17_mask.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA_HT_7690_19960312</td>\n",
       "      <td>TCGA_HT_7690_19960312_9.tif</td>\n",
       "      <td>TCGA_HT_7690_19960312_9_mask.tif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               directory                        images  \\\n",
       "0  TCGA_HT_7690_19960312  TCGA_HT_7690_19960312_16.tif   \n",
       "1  TCGA_HT_7690_19960312  TCGA_HT_7690_19960312_10.tif   \n",
       "2  TCGA_HT_7690_19960312  TCGA_HT_7690_19960312_18.tif   \n",
       "3  TCGA_HT_7690_19960312  TCGA_HT_7690_19960312_17.tif   \n",
       "4  TCGA_HT_7690_19960312   TCGA_HT_7690_19960312_9.tif   \n",
       "\n",
       "                               masks  \n",
       "0  TCGA_HT_7690_19960312_16_mask.tif  \n",
       "1  TCGA_HT_7690_19960312_10_mask.tif  \n",
       "2  TCGA_HT_7690_19960312_18_mask.tif  \n",
       "3  TCGA_HT_7690_19960312_17_mask.tif  \n",
       "4   TCGA_HT_7690_19960312_9_mask.tif  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs, images, masks = [], [], []\n",
    "\n",
    "i =0\n",
    "\n",
    "for root, folders, files in  os.walk(os.path.join(DATA_PATH, \"kaggle_3m/\")):\n",
    "    for file in files:\n",
    "        if 'mask' in file:\n",
    "            dirs.append(root.replace(os.path.join(DATA_PATH, \"kaggle_3m/\"), ''))\n",
    "            masks.append(file)\n",
    "            images.append(file.replace(\"_mask\", \"\"))\n",
    "\n",
    "PathDF = pd.DataFrame({'directory': dirs,\n",
    "                      'images': images,\n",
    "                      'masks': masks})\n",
    "PathDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, path_df, transform=None):\n",
    "        self.path_df = path_df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.path_df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        base_path = os.path.join(os.path.join(DATA_PATH, \"kaggle_3m/\"), self.path_df.iloc[idx]['directory'])\n",
    "        img_path = os.path.join(base_path, self.path_df.iloc[idx]['images'])\n",
    "        mask_path = os.path.join(base_path, self.path_df.iloc[idx]['masks'])\n",
    "        \n",
    "        image = Image.open(img_path)\n",
    "        mask = Image.open(mask_path)\n",
    "        \n",
    "        sample = (image, mask)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        image, mask = sample\n",
    "        # print(type(image), np.asarray(image).shape, type(mask), np.asarray(mask).shape)\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import the necessary packages\n",
    "# from torch.utils.data import Dataset\n",
    "# import cv2\n",
    "# class SegmentationDataset(Dataset):\n",
    "# \tdef __init__(self, imagePaths, maskPaths, transforms):\n",
    "# \t\t# store the image and mask filepaths, and augmentation\n",
    "# \t\t# transforms\n",
    "# \t\tself.imagePaths = imagePaths\n",
    "# \t\tself.maskPaths = maskPaths\n",
    "# \t\tself.transforms = transforms\n",
    "# \tdef __len__(self):\n",
    "# \t\t# return the number of total samples contained in the dataset\n",
    "# \t\treturn len(self.imagePaths)\n",
    "# \tdef __getitem__(self, idx):\n",
    "# \t\t# grab the image path from the current index\n",
    "# \t\timagePath = self.imagePaths[idx]\n",
    "# \t\t# load the image from disk, swap its channels from BGR to RGB,\n",
    "# \t\t# and read the associated mask from disk in grayscale mode\n",
    "# \t\timage = cv2.imread(imagePath)\n",
    "# \t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "# \t\tmask = cv2.imread(self.maskPaths[idx], 0)\n",
    "# \t\t# check to see if we are applying any transformations\n",
    "# \t\tif self.transforms is not None:\n",
    "# \t\t\t# apply the transformations to both image and its mask\n",
    "# \t\t\timage = self.transforms(image)\n",
    "# \t\t\tmask = self.transforms(mask)\n",
    "# \t\t# return a tuple of the image and its mask\n",
    "# \t\treturn (image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "# from . import config\n",
    "from torch.nn import ConvTranspose2d, Conv2d, MaxPool2d, Module, ModuleList, ReLU\n",
    "from torchvision.transforms import CenterCrop\n",
    "from torch.nn import functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(Module):\n",
    "\tdef __init__(self, inChannels, outChannels):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# store the convolution and RELU layers\n",
    "\t\tself.conv1 = Conv2d(inChannels, outChannels, 3)\n",
    "\t\tself.relu = ReLU()\n",
    "\t\tself.conv2 = Conv2d(outChannels, outChannels, 3)\n",
    "\tdef forward(self, x):\n",
    "\t\t# apply CONV => RELU => CONV block to the inputs and return it\n",
    "\t\treturn self.conv2(self.relu(self.conv1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Module):\n",
    "\tdef __init__(self, channels=(3, 16, 32, 64)):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# store the encoder blocks and maxpooling layer\n",
    "\t\tself.encBlocks = ModuleList(\n",
    "\t\t\t[Block(channels[i], channels[i + 1])\n",
    "\t\t\t \tfor i in range(len(channels) - 1)])\n",
    "\t\tself.pool = MaxPool2d(2)\n",
    "\tdef forward(self, x):\n",
    "\t\t# initialize an empty list to store the intermediate outputs\n",
    "\t\tblockOutputs = []\n",
    "\t\t# loop through the encoder blocks\n",
    "\t\tfor block in self.encBlocks:\n",
    "\t\t\t# pass the inputs through the current encoder block, store\n",
    "\t\t\t# the outputs, and then apply maxpooling on the output\n",
    "\t\t\tx = block(x)\n",
    "\t\t\tblockOutputs.append(x)\n",
    "\t\t\tx = self.pool(x)\n",
    "\t\t# return the list containing the intermediate outputs\n",
    "\t\treturn blockOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Module):\n",
    "\tdef __init__(self, channels=(64, 32, 16)):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# initialize the number of channels, upsampler blocks, and\n",
    "\t\t# decoder blocks\n",
    "\t\tself.channels = channels\n",
    "\t\tself.upconvs = ModuleList(\n",
    "\t\t\t[ConvTranspose2d(channels[i], channels[i + 1], 2, 2)\n",
    "\t\t\t \tfor i in range(len(channels) - 1)])\n",
    "\t\tself.dec_blocks = ModuleList(\n",
    "\t\t\t[Block(channels[i], channels[i + 1])\n",
    "\t\t\t \tfor i in range(len(channels) - 1)])\n",
    "\tdef forward(self, x, encFeatures):\n",
    "\t\t# loop through the number of channels\n",
    "\t\tfor i in range(len(self.channels) - 1):\n",
    "\t\t\t# pass the inputs through the upsampler blocks\n",
    "\t\t\tx = self.upconvs[i](x)\n",
    "\t\t\t# crop the current features from the encoder blocks,\n",
    "\t\t\t# concatenate them with the current upsampled features,\n",
    "\t\t\t# and pass the concatenated output through the current\n",
    "\t\t\t# decoder block\n",
    "\t\t\tencFeat = self.crop(encFeatures[i], x)\n",
    "\t\t\tx = torch.cat([x, encFeat], dim=1)\n",
    "\t\t\tx = self.dec_blocks[i](x)\n",
    "\t\t# return the final decoder output\n",
    "\t\treturn x\n",
    "\tdef crop(self, encFeatures, x):\n",
    "\t\t# grab the dimensions of the inputs, and crop the encoder\n",
    "\t\t# features to match the dimensions\n",
    "\t\t(_, _, H, W) = x.shape\n",
    "\t\tencFeatures = CenterCrop([H, W])(encFeatures)\n",
    "\t\t# return the cropped features\n",
    "\t\treturn encFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(Module):\n",
    "\tdef __init__(self, encChannels=(3, 16, 32, 64), decChannels=(64, 32, 16), nbClasses=1, retainDim=True, outSize=(INPUT_IMAGE_HEIGHT,  INPUT_IMAGE_WIDTH)):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# initialize the encoder and decoder\n",
    "\t\tself.encoder = Encoder(encChannels)\n",
    "\t\tself.decoder = Decoder(decChannels)\n",
    "\t\t# initialize the regression head and store the class variables\n",
    "\t\tself.head = Conv2d(decChannels[-1], nbClasses, 1)\n",
    "\t\tself.retainDim = retainDim\n",
    "\t\tself.outSize = outSize\n",
    "\tdef forward(self, x):\n",
    "\t\t# grab the features from the encoder\n",
    "\t\tencFeatures = self.encoder(x)\n",
    "\t\t# pass the encoder features through decoder making sure that\n",
    "\t\t# their dimensions are suited for concatenation\n",
    "\t\tdecFeatures = self.decoder(encFeatures[::-1][0],\n",
    "\t\t\tencFeatures[::-1][1:])\n",
    "\t\t# pass the decoder features through the regression head to\n",
    "\t\t# obtain the segmentation mask\n",
    "\t\tmap = self.head(decFeatures)\n",
    "\t\t# check to see if we are retaining the original output\n",
    "\t\t# dimensions and if so, then resize the output to match them\n",
    "\t\tif self.retainDim:\n",
    "\t\t\tmap = F.interpolate(map, self.outSize)\n",
    "\t\t# return the segmentation map\n",
    "\t\treturn map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE\n",
    "# python train.py\n",
    "# import the necessary packages\n",
    "# from pyimagesearch.dataset import SegmentationDataset\n",
    "# from pyimagesearch.model import UNet\n",
    "# from pyimagesearch import config\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "# from imutils import paths\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(PathDF, random_state=42, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image and mask filepaths in a sorted manner\n",
    "# imagePaths = sorted(list(paths.list_images(IMAGE_DATASET_PATH)))\n",
    "# maskPaths = sorted(list(paths.list_images(MASK_DATASET_PATH)))\n",
    "# partition the data into training and testing splits using 85% of\n",
    "# the data for training and the remaining 15% for testing\n",
    "# split = train_test_split(imagePaths, maskPaths,\n",
    "# \ttest_size=config.TEST_SPLIT, random_state=42)\n",
    "# # unpack the data split\n",
    "# (trainImages, testImages) = split[:2]\n",
    "# (trainMasks, testMasks) = split[2:]\n",
    "# # write the testing image paths to disk so that we can use then\n",
    "# # when evaluating/testing our model\n",
    "# print(\"[INFO] saving testing image paths...\")\n",
    "# f = open(config.TEST_PATHS, \"w\")\n",
    "# f.write(\"\\n\".join(testImages))\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SegmentationDataset(train_df)\n",
    "test_data = SegmentationDataset(test_df)\n",
    "\n",
    "trainLoader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
    "testLoader = DataLoader(test_data, batch_size=8, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define transformations\n",
    "# transforms = transforms.Compose([transforms.ToPILImage(),\n",
    "#  \ttransforms.Resize((INPUT_IMAGE_HEIGHT,\n",
    "# \t\tINPUT_IMAGE_WIDTH)),\n",
    "# \ttransforms.ToTensor()])\n",
    "# # create the train and test datasets\n",
    "# trainDS = SegmentationDataset(imagePaths=trainImages, maskPaths=trainMasks,\n",
    "# \ttransforms=transforms)\n",
    "# testDS = SegmentationDataset(imagePaths=testImages, maskPaths=testMasks,\n",
    "#     transforms=transforms)\n",
    "# print(f\"[INFO] found {len(trainDS)} examples in the training set...\")\n",
    "# print(f\"[INFO] found {len(testDS)} examples in the test set...\")\n",
    "# # create the training and test data loaders\n",
    "# trainLoader = DataLoader(trainDS, shuffle=True,\n",
    "# \tbatch_size=config.BATCH_SIZE, pin_memory=config.PIN_MEMORY,\n",
    "# \tnum_workers=os.cpu_count())\n",
    "# testLoader = DataLoader(testDS, shuffle=False,\n",
    "# \tbatch_size=config.BATCH_SIZE, pin_memory=config.PIN_MEMORY,\n",
    "# \tnum_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our UNet model\n",
    "unet = UNet().to(DEVICE)\n",
    "# initialize loss function and optimizer\n",
    "lossFunc = BCEWithLogitsLoss()\n",
    "opt = Adam(unet.parameters(), lr=INIT_LR)\n",
    "# calculate steps per epoch for training and test set\n",
    "trainSteps = len(train_data) // BATCH_SIZE\n",
    "testSteps = len(test_data) // BATCH_SIZE\n",
    "# initialize a dictionary to store training history\n",
    "H = {\"train_loss\": [], \"test_loss\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/yasaisen/.local/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/yasaisen/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 61, in fetch\n    return self.collate_fn(data)\n  File \"/home/yasaisen/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/yasaisen/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 143, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/yasaisen/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 143, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/yasaisen/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 151, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.TiffImagePlugin.TiffImageFile'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m totalTestLoss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[39m# loop over the training set\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfor\u001b[39;00m (i, (x, y)) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainLoader):\n\u001b[1;32m     12\u001b[0m \t\u001b[39m# send the input to the device\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \t(x, y) \u001b[39m=\u001b[39m (x\u001b[39m.\u001b[39mto(DEVICE), y\u001b[39m.\u001b[39mto(DEVICE))\n\u001b[1;32m     14\u001b[0m \t\u001b[39m# perform a forward pass and calculate the training loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1333\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1333\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1359\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1360\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 543\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/yasaisen/.local/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/yasaisen/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 61, in fetch\n    return self.collate_fn(data)\n  File \"/home/yasaisen/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/yasaisen/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 143, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/yasaisen/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 143, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/yasaisen/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 151, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.TiffImagePlugin.TiffImageFile'>\n"
     ]
    }
   ],
   "source": [
    "# loop over epochs\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "for e in tqdm(range(NUM_EPOCHS)):\n",
    "\t# set the model in training mode\n",
    "\tunet.train()\n",
    "\t# initialize the total training and validation loss\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalTestLoss = 0\n",
    "\t# loop over the training set\n",
    "\tfor (i, (x, y)) in enumerate(trainLoader):\n",
    "\t\t# send the input to the device\n",
    "\t\t(x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "\t\t# perform a forward pass and calculate the training loss\n",
    "\t\tpred = unet(x)\n",
    "\t\tloss = lossFunc(pred, y)\n",
    "\t\t# first, zero out any previously accumulated gradients, then\n",
    "\t\t# perform backpropagation, and then update model parameters\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\t\t# add the loss to the total training loss so far\n",
    "\t\ttotalTrainLoss += loss\n",
    "\t# switch off autograd\n",
    "\twith torch.no_grad():\n",
    "\t\t# set the model in evaluation mode\n",
    "\t\tunet.eval()\n",
    "\t\t# loop over the validation set\n",
    "\t\tfor (x, y) in testLoader:\n",
    "\t\t\t# send the input to the device\n",
    "\t\t\t(x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "\t\t\t# make the predictions and calculate the validation loss\n",
    "\t\t\tpred = unet(x)\n",
    "\t\t\ttotalTestLoss += lossFunc(pred, y)\n",
    "\t# calculate the average training and validation loss\n",
    "\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\tavgTestLoss = totalTestLoss / testSteps\n",
    "\t# update our training history\n",
    "\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\tH[\"test_loss\"].append(avgTestLoss.cpu().detach().numpy())\n",
    "\t# print the model training and validation information\n",
    "\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
    "\tprint(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(\n",
    "\t\tavgTrainLoss, avgTestLoss))\n",
    "# display the total time needed to perform the training\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "\tendTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"test_loss\"], label=\"test_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(config.PLOT_PATH)\n",
    "# serialize the model to disk\n",
    "torch.save(unet, config.MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE\n",
    "# python predict.py\n",
    "# import the necessary packages\n",
    "from pyimagesearch import config\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "def prepare_plot(origImage, origMask, predMask):\n",
    "\t# initialize our figure\n",
    "\tfigure, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 10))\n",
    "\t# plot the original image, its mask, and the predicted mask\n",
    "\tax[0].imshow(origImage)\n",
    "\tax[1].imshow(origMask)\n",
    "\tax[2].imshow(predMask)\n",
    "\t# set the titles of the subplots\n",
    "\tax[0].set_title(\"Image\")\n",
    "\tax[1].set_title(\"Original Mask\")\n",
    "\tax[2].set_title(\"Predicted Mask\")\n",
    "\t# set the layout of the figure and display it\n",
    "\tfigure.tight_layout()\n",
    "\tfigure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, imagePath):\n",
    "\t# set model to evaluation mode\n",
    "\tmodel.eval()\n",
    "\t# turn off gradient tracking\n",
    "\twith torch.no_grad():\n",
    "\t\t# load the image from disk, swap its color channels, cast it\n",
    "\t\t# to float data type, and scale its pixel values\n",
    "\t\timage = cv2.imread(imagePath)\n",
    "\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\t\timage = image.astype(\"float32\") / 255.0\n",
    "\t\t# resize the image and make a copy of it for visualization\n",
    "\t\timage = cv2.resize(image, (128, 128))\n",
    "\t\torig = image.copy()\n",
    "\t\t# find the filename and generate the path to ground truth\n",
    "\t\t# mask\n",
    "\t\tfilename = imagePath.split(os.path.sep)[-1]\n",
    "\t\tgroundTruthPath = os.path.join(config.MASK_DATASET_PATH,\n",
    "\t\t\tfilename)\n",
    "\t\t# load the ground-truth segmentation mask in grayscale mode\n",
    "\t\t# and resize it\n",
    "\t\tgtMask = cv2.imread(groundTruthPath, 0)\n",
    "\t\tgtMask = cv2.resize(gtMask, (config.INPUT_IMAGE_HEIGHT,\n",
    "\t\t\tconfig.INPUT_IMAGE_HEIGHT))\n",
    "\t\t\n",
    "\t\t# make the channel axis to be the leading one, add a batch\n",
    "\t\t# dimension, create a PyTorch tensor, and flash it to the\n",
    "\t\t# current device\n",
    "\t\timage = np.transpose(image, (2, 0, 1))\n",
    "\t\timage = np.expand_dims(image, 0)\n",
    "\t\timage = torch.from_numpy(image).to(config.DEVICE)\n",
    "\t\t# make the prediction, pass the results through the sigmoid\n",
    "\t\t# function, and convert the result to a NumPy array\n",
    "\t\tpredMask = model(image).squeeze()\n",
    "\t\tpredMask = torch.sigmoid(predMask)\n",
    "\t\tpredMask = predMask.cpu().numpy()\n",
    "\t\t# filter out the weak predictions and convert them to integers\n",
    "\t\tpredMask = (predMask > config.THRESHOLD) * 255\n",
    "\t\tpredMask = predMask.astype(np.uint8)\n",
    "\t\t# prepare a plot for visualization\n",
    "\t\tprepare_plot(orig, gtMask, predMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image paths in our testing file and randomly select 10\n",
    "# image paths\n",
    "print(\"[INFO] loading up test image paths...\")\n",
    "imagePaths = open(config.TEST_PATHS).read().strip().split(\"\\n\")\n",
    "imagePaths = np.random.choice(imagePaths, size=10)\n",
    "# load our model from disk and flash it to the current device\n",
    "print(\"[INFO] load up model...\")\n",
    "unet = torch.load(config.MODEL_PATH).to(config.DEVICE)\n",
    "# iterate over the randomly selected test image paths\n",
    "for path in imagePaths:\n",
    "\t# make predictions and visualize the results\n",
    "\tmake_predictions(unet, path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

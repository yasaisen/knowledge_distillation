{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Necessary imports\n",
    "import torch\n",
    "# from torch import nn\n",
    "# import torch.nn.functional as F\n",
    "# import torchvision\n",
    "# from torchvision import transforms\n",
    "# from torchinfo import summary\n",
    "# from tqdm import tqdm\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import cv2\n",
    "# from PIL import Image\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "# torch.manual_seed(0)\n",
    "# torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_dir = '/home/yasaisen/Desktop/10_research/research_main/lab_01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \n",
    "    data_dir = os.path.join(root_dir, \"kaggle_3m/\")\n",
    "    \n",
    "    validation_fraction = 0.15\n",
    "    test_fraction = 0.10\n",
    "    train_batch = 16\n",
    "    valid_batch = 32\n",
    "    test_batch = 32\n",
    "    \n",
    "    input_dim = 256\n",
    "    input_ch = 3\n",
    "    output_dim = 256\n",
    "    output_ch = 1\n",
    "    \n",
    "    seed = 1\n",
    "    learning_rate = 0.01\n",
    "    epochs = 30\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "def plot_example(idx):\n",
    "    base_path = Config.data_dir+ test_df['directory'].iloc[idx]\n",
    "    img_path = os.path.join(base_path, test_df['images'].iloc[idx])\n",
    "    mask_path = os.path.join(base_path, test_df['masks'].iloc[idx])\n",
    "    img = Image.open(img_path)\n",
    "    mask = Image.open(mask_path)\n",
    "    print(np.max(np.asarray(mask)))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8,4))\n",
    "    ax[0].imshow(img)\n",
    "    ax[0].set_title(\"Image\")\n",
    "    ax[1].imshow(mask)\n",
    "    ax[1].set_title(\"Mask\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(sample, title=None):\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(sample[0])\n",
    "    ax[1].imshow(sample[1], cmap=\"gray\")\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(model, idx):\n",
    "    base_path = os.path.join(Config.data_dir, test_df['directory'].iloc[idx])\n",
    "    img_path = os.path.join(base_path, test_df['images'].iloc[idx])\n",
    "    mask_path = os.path.join(base_path, test_df['masks'].iloc[idx])\n",
    "    \n",
    "    img = Image.open(img_path)\n",
    "    mask = Image.open(mask_path)\n",
    "    \n",
    "    tensor_img, tensor_mask = eval_transforms((img, mask))\n",
    "    tensor_img = tensor_img.unsqueeze(0).to(Config.device)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(tensor_img)[0].detach().cpu().numpy()\n",
    "        pred = pred.transpose((1,2,0)).squeeze()\n",
    "        rounded = np.round(pred)\n",
    "        \n",
    "    plot_images = {\n",
    "        'Image': img,\n",
    "        'Mask': mask,\n",
    "        'Predicted Mask': pred,\n",
    "        'Predicted Rounded Mask':rounded\n",
    "    }\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 4, figsize=(16,4))\n",
    "    for i, key in enumerate(plot_images.keys()):\n",
    "        ax[i].imshow(plot_images[key])\n",
    "        ax[i].set_title(key)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(Config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>directory</th>\n",
       "      <th>images</th>\n",
       "      <th>masks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA_HT_7690_19960312</td>\n",
       "      <td>TCGA_HT_7690_19960312_16.tif</td>\n",
       "      <td>TCGA_HT_7690_19960312_16_mask.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA_HT_7690_19960312</td>\n",
       "      <td>TCGA_HT_7690_19960312_10.tif</td>\n",
       "      <td>TCGA_HT_7690_19960312_10_mask.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA_HT_7690_19960312</td>\n",
       "      <td>TCGA_HT_7690_19960312_18.tif</td>\n",
       "      <td>TCGA_HT_7690_19960312_18_mask.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA_HT_7690_19960312</td>\n",
       "      <td>TCGA_HT_7690_19960312_17.tif</td>\n",
       "      <td>TCGA_HT_7690_19960312_17_mask.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA_HT_7690_19960312</td>\n",
       "      <td>TCGA_HT_7690_19960312_9.tif</td>\n",
       "      <td>TCGA_HT_7690_19960312_9_mask.tif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               directory                        images  \\\n",
       "0  TCGA_HT_7690_19960312  TCGA_HT_7690_19960312_16.tif   \n",
       "1  TCGA_HT_7690_19960312  TCGA_HT_7690_19960312_10.tif   \n",
       "2  TCGA_HT_7690_19960312  TCGA_HT_7690_19960312_18.tif   \n",
       "3  TCGA_HT_7690_19960312  TCGA_HT_7690_19960312_17.tif   \n",
       "4  TCGA_HT_7690_19960312   TCGA_HT_7690_19960312_9.tif   \n",
       "\n",
       "                               masks  \n",
       "0  TCGA_HT_7690_19960312_16_mask.tif  \n",
       "1  TCGA_HT_7690_19960312_10_mask.tif  \n",
       "2  TCGA_HT_7690_19960312_18_mask.tif  \n",
       "3  TCGA_HT_7690_19960312_17_mask.tif  \n",
       "4   TCGA_HT_7690_19960312_9_mask.tif  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs, images, masks = [], [], []\n",
    "\n",
    "i =0\n",
    "\n",
    "for root, folders, files in  os.walk(Config.data_dir):\n",
    "    for file in files:\n",
    "        if 'mask' in file:\n",
    "            dirs.append(root.replace(Config.data_dir, ''))\n",
    "            masks.append(file)\n",
    "            images.append(file.replace(\"_mask\", \"\"))\n",
    "\n",
    "PathDF = pd.DataFrame({'directory': dirs,\n",
    "                      'images': images,\n",
    "                      'masks': masks})\n",
    "PathDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in PathDF.index:\n",
    "    PathDF.loc[i, \"diagnosis\"] = 1 if np.max(cv2.imread(os.path.join(Config.data_dir, PathDF.loc[i, 'directory'], PathDF.loc[i,\"masks\"]))) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>directory</th>\n",
       "      <th>images</th>\n",
       "      <th>masks</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA_HT_7690_19960312</td>\n",
       "      <td>TCGA_HT_7690_19960312_16.tif</td>\n",
       "      <td>TCGA_HT_7690_19960312_16_mask.tif</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA_HT_7690_19960312</td>\n",
       "      <td>TCGA_HT_7690_19960312_10.tif</td>\n",
       "      <td>TCGA_HT_7690_19960312_10_mask.tif</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA_HT_7690_19960312</td>\n",
       "      <td>TCGA_HT_7690_19960312_18.tif</td>\n",
       "      <td>TCGA_HT_7690_19960312_18_mask.tif</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA_HT_7690_19960312</td>\n",
       "      <td>TCGA_HT_7690_19960312_17.tif</td>\n",
       "      <td>TCGA_HT_7690_19960312_17_mask.tif</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA_HT_7690_19960312</td>\n",
       "      <td>TCGA_HT_7690_19960312_9.tif</td>\n",
       "      <td>TCGA_HT_7690_19960312_9_mask.tif</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               directory                        images  \\\n",
       "0  TCGA_HT_7690_19960312  TCGA_HT_7690_19960312_16.tif   \n",
       "1  TCGA_HT_7690_19960312  TCGA_HT_7690_19960312_10.tif   \n",
       "2  TCGA_HT_7690_19960312  TCGA_HT_7690_19960312_18.tif   \n",
       "3  TCGA_HT_7690_19960312  TCGA_HT_7690_19960312_17.tif   \n",
       "4  TCGA_HT_7690_19960312   TCGA_HT_7690_19960312_9.tif   \n",
       "\n",
       "                               masks  diagnosis  \n",
       "0  TCGA_HT_7690_19960312_16_mask.tif        1.0  \n",
       "1  TCGA_HT_7690_19960312_10_mask.tif        0.0  \n",
       "2  TCGA_HT_7690_19960312_18_mask.tif        1.0  \n",
       "3  TCGA_HT_7690_19960312_17_mask.tif        1.0  \n",
       "4   TCGA_HT_7690_19960312_9_mask.tif        0.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PathDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    2556\n",
       "1.0    1373\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PathDF['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = PathDF['diagnosis'].value_counts().plot(kind='bar', stacked=True, figsize=(10,6), color=['blue', 'red'])\n",
    "ax.set_title('Data Distribution', fontsize=15)\n",
    "ax.set_ylabel('Total Images', fontsize=15)\n",
    "ax.set_xticklabels(['No Tumor', 'Tumor'], fontsize=12, rotation=0)\n",
    "for i, rows in enumerate(PathDF['diagnosis'].value_counts().values):\n",
    "    ax.annotate(int(rows), xy=(i, rows+12), ha='center', fontweight='bold', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2946\n",
      "Valid: 590\n",
      "Test: 393\n"
     ]
    }
   ],
   "source": [
    "train2rest = Config.validation_fraction + Config.test_fraction\n",
    "test2valid = Config.validation_fraction / train2rest\n",
    "\n",
    "train_df, rest = train_test_split(PathDF, random_state=Config.seed,\n",
    "                                 test_size = train2rest)\n",
    "\n",
    "test_df, valid_df = train_test_split(rest, random_state=Config.seed,\n",
    "                                    test_size = test2valid)\n",
    "\n",
    "print('Train:', train_df.shape[0])\n",
    "print('Valid:', valid_df.shape[0])\n",
    "print('Test:', test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRI_Dataset(Dataset):\n",
    "    def __init__(self, path_df, transform=None):\n",
    "        self.path_df = path_df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.path_df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        base_path = os.path.join(Config.data_dir, self.path_df.iloc[idx]['directory'])\n",
    "        img_path = os.path.join(base_path, self.path_df.iloc[idx]['images'])\n",
    "        mask_path = os.path.join(base_path, self.path_df.iloc[idx]['masks'])\n",
    "        \n",
    "        image = Image.open(img_path)\n",
    "        mask = Image.open(mask_path)\n",
    "        \n",
    "        sample = (image, mask)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedRandomHorizontalFlip():\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        img, mask = sample\n",
    "        if np.random.random() < self.p:\n",
    "            img, mask = TF.hflip(img), TF.hflip(mask)\n",
    "            \n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedRandomAffine():\n",
    "    def __init__(self, degrees= None, translate=None, scale_ranges=None,\n",
    "                shears=None):\n",
    "        self.params = {\n",
    "            'degree': degrees,\n",
    "            'translate': translate,\n",
    "            'scale_ranges':scale_ranges,\n",
    "            'shears':shears\n",
    "        }\n",
    "    def __call__(self, sample):\n",
    "        img, mask = sample\n",
    "        w, h = img.size\n",
    "        \n",
    "        angle, translations, scale, shear = transforms.RandomAffine.get_params(\n",
    "            self.params['degree'], self.params['translate'],\n",
    "            self.params['scale_ranges'], self.params['shears'],\n",
    "            (w,h)\n",
    "        )\n",
    "        \n",
    "        img = TF.affine(img, angle, translations, scale, shear)\n",
    "        mask = TF.affine(mask, angle, translations, scale, shear)\n",
    "        \n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedToTensor():\n",
    "    def __call__(self, sample):\n",
    "        img, mask = sample\n",
    "        img = np.array(img)\n",
    "        mask = np.expand_dims(mask, -1)\n",
    "        img = np.moveaxis(img, -1, 0)\n",
    "        mask = np.moveaxis(mask, -1, 0)\n",
    "        img, mask = torch.FloatTensor(img), torch.FloatTensor(mask)\n",
    "        img = img/255\n",
    "        mask = mask/255\n",
    "        return img, mask\n",
    "dataset = MRI_Dataset(test_df)\n",
    "sample = dataset[59]\n",
    "transform = PairedRandomHorizontalFlip(p=1)\n",
    "transform = PairedRandomAffine(\n",
    "    degrees = (15,15),\n",
    "    scale_ranges = (1.2, 1.2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose(\n",
    "    [PairedRandomHorizontalFlip(),\n",
    "    PairedRandomAffine(\n",
    "        degrees=(-15, 15),\n",
    "        translate=(0.1, 0.1),\n",
    "        scale_ranges=(0.8, 1.2)\n",
    "    ),\n",
    "    PairedToTensor()\n",
    "    ])\n",
    "\n",
    "eval_transforms = PairedToTensor()\n",
    "\n",
    "train_data = MRI_Dataset(train_df, transform=train_transforms)\n",
    "valid_data = MRI_Dataset(valid_df, transform=eval_transforms)\n",
    "test_data = MRI_Dataset(test_df, transform=eval_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=Config.train_batch,\n",
    "                         shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_data, batch_size=Config.valid_batch,\n",
    "                         shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=Config.test_batch,\n",
    "                        shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(    \n",
    "    in_channels=Config.input_ch,                  \n",
    "    classes=Config.output_ch,\n",
    "    activation=\"sigmoid\").to(Config.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42140/561252387.py:8: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  img, mask = torch.FloatTensor(img), torch.FloatTensor(mask)\n",
      "/tmp/ipykernel_42140/561252387.py:8: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  img, mask = torch.FloatTensor(img), torch.FloatTensor(mask)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 256, 256])\n",
      "torch.Size([16, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 256, 256])\n",
      "torch.Size([16, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(model)\n",
    "t = torch.randn((16, 3, 256, 256)).cuda()\n",
    "print(t.shape)\n",
    "get = model(t)\n",
    "print(get.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model, device):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            _, y = y.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    model.train()\n",
    "    return (num_correct/num_samples).item()\n",
    "  \n",
    "\n",
    "def train(epochs, model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "\n",
    "        pbar = tqdm(train_loader, total=len(train_loader), position=0, leave=True, desc=f\"Epoch {epoch}\")\n",
    "        for data, targets in pbar:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward\n",
    "            scores = model(data)\n",
    "            loss = criterion(scores, targets)\n",
    "            losses.append(loss.item() * data.shape[0])\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        acc = check_accuracy(test_loader, model, device)\n",
    "        print(f\"Loss:{avg_loss:.8f}\\tAccuracy:{acc:.8f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    teacher_model,\n",
    "    student_model,\n",
    "    optimizer,\n",
    "    student_loss_fn,\n",
    "    divergence_loss_fn,\n",
    "    temp,\n",
    "    alpha,\n",
    "    epoch,\n",
    "    device\n",
    "):\n",
    "    losses = []\n",
    "    pbar = tqdm(train_loader, total=len(train_loader), position=0, leave=True, desc=f\"Epoch {epoch}\")\n",
    "    for data, targets in pbar:\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward\n",
    "        with torch.no_grad():\n",
    "            teacher_preds = teacher_model(data)\n",
    "\n",
    "        student_preds = student_model(data)\n",
    "        student_loss = student_loss_fn(student_preds, targets)\n",
    "        \n",
    "        ditillation_loss = divergence_loss_fn(\n",
    "            F.log_softmax(student_preds / temp, dim=1),\n",
    "            F.softmax(teacher_preds / temp, dim=1)\n",
    "        )\n",
    "        loss = alpha * student_loss + (1 - alpha) * ditillation_loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    return avg_loss\n",
    "  \n",
    "def main(epochs, teacher, student, temp=7, alpha=0.3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    teacher = teacher.to(device)\n",
    "    student = student.to(device)\n",
    "    student_loss_fn = nn.CrossEntropyLoss()\n",
    "    divergence_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    optimizer = torch.optim.Adam(student.parameters(), lr=1e-4)\n",
    "\n",
    "    teacher.eval()\n",
    "    student.train()\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_step(\n",
    "            teacher,\n",
    "            student,\n",
    "            optimizer,\n",
    "            student_loss_fn,\n",
    "            divergence_loss_fn,\n",
    "            temp,\n",
    "            alpha,\n",
    "            epoch,\n",
    "            device\n",
    "        )\n",
    "        acc = check_accuracy(test_loader, student, device)\n",
    "        print(f\"Loss:{loss:.8f}\\tAccuracy:{acc:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/185 [00:00<?, ?it/s]/tmp/ipykernel_42140/561252387.py:8: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  img, mask = torch.FloatTensor(img), torch.FloatTensor(mask)\n",
      "/tmp/ipykernel_42140/561252387.py:8: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  img, mask = torch.FloatTensor(img), torch.FloatTensor(mask)\n",
      "Epoch 0: 100%|| 185/185 [00:46<00:00,  3.96it/s]\n",
      "/tmp/ipykernel_42140/561252387.py:8: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  img, mask = torch.FloatTensor(img), torch.FloatTensor(mask)\n",
      "/tmp/ipykernel_42140/561252387.py:8: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  img, mask = torch.FloatTensor(img), torch.FloatTensor(mask)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:5.22105756\tAccuracy:65536.00000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/185 [00:00<?, ?it/s]/tmp/ipykernel_42140/561252387.py:8: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  img, mask = torch.FloatTensor(img), torch.FloatTensor(mask)\n",
      "/tmp/ipykernel_42140/561252387.py:8: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  img, mask = torch.FloatTensor(img), torch.FloatTensor(mask)\n",
      "Epoch 1: 100%|| 185/185 [00:46<00:00,  3.94it/s]\n",
      "/tmp/ipykernel_42140/561252387.py:8: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  img, mask = torch.FloatTensor(img), torch.FloatTensor(mask)\n",
      "/tmp/ipykernel_42140/561252387.py:8: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  img, mask = torch.FloatTensor(img), torch.FloatTensor(mask)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:2.45160787\tAccuracy:65536.00000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/185 [00:00<?, ?it/s]/tmp/ipykernel_42140/561252387.py:8: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  img, mask = torch.FloatTensor(img), torch.FloatTensor(mask)\n",
      "/tmp/ipykernel_42140/561252387.py:8: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  img, mask = torch.FloatTensor(img), torch.FloatTensor(mask)\n",
      "Epoch 2: 100%|| 185/185 [01:38<00:00,  1.88it/s]\n",
      "/tmp/ipykernel_42140/561252387.py:8: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  img, mask = torch.FloatTensor(img), torch.FloatTensor(mask)\n",
      "/tmp/ipykernel_42140/561252387.py:8: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  img, mask = torch.FloatTensor(img), torch.FloatTensor(mask)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.55007862\tAccuracy:65536.00000000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ROOT_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trained_model \u001b[39m=\u001b[39m train(\u001b[39m3\u001b[39m, model)\n\u001b[0;32m----> 2\u001b[0m torch\u001b[39m.\u001b[39msave({\u001b[39m'\u001b[39m\u001b[39mstate_dict\u001b[39m\u001b[39m'\u001b[39m: trained_model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m : trained_model,}, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(ROOT_PATH, \u001b[39m'\u001b[39m\u001b[39mresnet18.pth\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ROOT_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "trained_model = train(3, model)\n",
    "torch.save({'state_dict': trained_model.state_dict(), 'model' : trained_model,}, os.path.join(ROOT_PATH, 'resnet18.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet34_model = resnet34(2)\n",
    "# trained_resnet34_model = train(3, resnet34_model)\n",
    "# torch.save({'state_dict': trained_resnet34_model.state_dict(), 'model' : trained_resnet34_model,}, os.path.join(ROOT_PATH, 'resnet34.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_resnet18_model = torch.load(os.path.join(ROOT_PATH, 'resnet18.pth'))['model']\n",
    "# trained_resnet34_model = torch.load(os.path.join(ROOT_PATH, 'resnet34.pth'))['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 2343/2343 [01:24<00:00, 27.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.03518395\tAccuracy:0.82119995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|| 2343/2343 [01:25<00:00, 27.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.00995989\tAccuracy:0.88239998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|| 2343/2343 [01:26<00:00, 26.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.00434331\tAccuracy:0.89719999\n"
     ]
    }
   ],
   "source": [
    "# # TE=34, ST=18\n",
    "\n",
    "# trained_resnet34_model = torch.load(os.path.join(ROOT_PATH, 'resnet34.pth'))['model']\n",
    "# trained_resnet34_model = trained_resnet34_model.cuda()\n",
    "\n",
    "# student_model = resnet18(2)\n",
    "# student_model = student_model.cuda()\n",
    "\n",
    "# main(3, trained_resnet34_model, student_model, alpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 2343/2343 [01:55<00:00, 20.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.03726752\tAccuracy:0.80879998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|| 2343/2343 [01:55<00:00, 20.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.01042126\tAccuracy:0.88479996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|| 2343/2343 [01:55<00:00, 20.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.00555547\tAccuracy:0.89800000\n"
     ]
    }
   ],
   "source": [
    "# # TE=18, ST=34\n",
    "\n",
    "# trained_resnet18_model = torch.load(os.path.join(ROOT_PATH, 'resnet18.pth'))['model'].cuda()\n",
    "\n",
    "# student_model = resnet34(2).cuda()\n",
    "# main(3, trained_resnet18_model, student_model, alpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 2343/2343 [02:07<00:00, 18.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.03669881\tAccuracy:0.82440001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|| 2343/2343 [02:07<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.01010854\tAccuracy:0.89879996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|| 2343/2343 [02:08<00:00, 18.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.00454724\tAccuracy:0.90840000\n"
     ]
    }
   ],
   "source": [
    "# # TE=34, ST=34\n",
    "\n",
    "# trained_resnet34_model = torch.load(os.path.join(ROOT_PATH, 'resnet34.pth'))['model'].cuda()\n",
    "\n",
    "# student_model = resnet34(2).cuda()\n",
    "# main(3, trained_resnet34_model, student_model, alpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 2343/2343 [01:56<00:00, 20.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.03405485\tAccuracy:0.82519996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|| 2343/2343 [01:16<00:00, 30.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.00901391\tAccuracy:0.88000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|| 2343/2343 [02:18<00:00, 16.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.00451345\tAccuracy:0.88999999\n"
     ]
    }
   ],
   "source": [
    "# TE=18, ST=18\n",
    "\n",
    "# trained_resnet18_model = torch.load(os.path.join(ROOT_PATH, 'resnet18.pth'))['model'].cuda()\n",
    "\n",
    "# student_model = resnet18(2).cuda()\n",
    "# main(3, trained_resnet18_model, student_model, alpha=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
